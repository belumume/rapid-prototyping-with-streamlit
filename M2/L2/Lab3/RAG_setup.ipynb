{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "ugdl2ehdiepzwfzehfl3",
   "authorId": "6841714608330",
   "authorName": "CHANINN",
   "authorEmail": "chanin.nantasenamat@snowflake.com",
   "sessionId": "e91f17a2-c7cd-45ab-a059-02b6481785fd",
   "lastEditTime": 1743789238663
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38532d44-0ee8-48d3-b9d6-760c29ce0ff8",
   "metadata": {
    "name": "md_title",
    "collapsed": false
   },
   "source": "# Setting up the RAG Pipeline using Cortex Search\n\nIn this notebook, we're setting up the RAG pipeline using the Avalanche customer review data from the previously prepared data stored in the stage. \n\nBy the end of the tutorial, we'll have a RAG pipeline ready to allow us to ask any questions about the data."
  },
  {
   "cell_type": "markdown",
   "id": "86d33a5a-ab8a-4a97-bc4c-0cfab8179d50",
   "metadata": {
    "name": "md_list_data",
    "collapsed": false
   },
   "source": "## List staged data\n\nHere, we're listing the contents of the staged data at `@avalanche.customer_reviews`"
  },
  {
   "cell_type": "code",
   "id": "bd1b575d-341f-48c6-95d0-0ec6bee89c78",
   "metadata": {
    "language": "sql",
    "name": "sql_list_data"
   },
   "outputs": [],
   "source": "ls @avalanche.customer_reviews;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "721b4bdd-fec8-4d48-9e5b-56aafc52a54d",
   "metadata": {
    "name": "md_parse_content",
    "collapsed": false
   },
   "source": "## Parse content\n\nHere, we're extracting or parsing content from the `DOCX` files stored in stage by leveraging the Cortex `PARSE_DOCUMENT()` function.\n\nThe parsed content will be stored at a newly created table at  `AVALANCHE.AVALANCHE.PARSED_CONTENT`."
  },
  {
   "cell_type": "code",
   "id": "4170f929-6275-4055-b025-6c760ee7e109",
   "metadata": {
    "language": "sql",
    "name": "sql_parse_content",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE AVALANCHE.AVALANCHE.PARSED_CONTENT AS SELECT \n      relative_path,\n      TO_VARCHAR(\n        SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n          @avalanche.customer_reviews, \n          relative_path, \n          {'mode': 'LAYOUT'}\n        ) :content\n      ) AS parsed_text\n    FROM directory(@avalanche.customer_reviews)\n    WHERE relative_path LIKE '%.docx'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "be400d91-e883-42d7-b26e-eb78a7219479",
   "metadata": {
    "language": "sql",
    "name": "cell5",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM AVALANCHE.AVALANCHE.PARSED_CONTENT LIMIT 5\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1004e517-c9d3-4217-b091-74fc5750865f",
   "metadata": {
    "language": "sql",
    "name": "cell2",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE AVALANCHE.AVALANCHE.CHUNKED_CONTENT (\n    file_name VARCHAR,\n    CHUNK VARCHAR\n);\n\nINSERT INTO AVALANCHE.AVALANCHE.CHUNKED_CONTENT (file_name, CHUNK)\nSELECT\n    relative_path,\n    c.value AS CHUNK\nFROM\n    AVALANCHE.AVALANCHE.PARSED_CONTENT,\n    LATERAL FLATTEN( input => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER (\n        parsed_text,\n        'markdown',\n        1800,\n        250\n    )) c;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42c3fd5b-5002-4b46-b0d7-232327da5fc4",
   "metadata": {
    "language": "sql",
    "name": "cell3",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM AVALANCHE.AVALANCHE.CHUNKED_CONTENT LIMIT 10",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "72983c92-cd1f-4b6d-bc06-c65b038f3999",
   "metadata": {
    "language": "sql",
    "name": "cell6",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE CORTEX SEARCH SERVICE AVALANCHE.AVALANCHE.AVALANCHE_SEARCH_SERVICE\n    ON chunk\n    WAREHOUSE = chanin_xs\n    TARGET_LAG = '1 minute'\n    EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n    AS (\n    SELECT\n        file_name,\n        chunk\n    FROM AVALANCHE.AVALANCHE.CHUNKED_CONTENT\n    );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "988ea569-dba2-402e-a727-ecb28d10e789",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "\n--query it with SQL\nSELECT PARSE_JSON(\n  SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n      'AVALANCHE.AVALANCHE.AVALANCHE_SEARCH_SERVICE',\n      '{\n         \"query\": \"Any goggles review?\",\n         \"columns\":[\n            \"file_name\",\n            \"CHUNK\"\n         ],\n         \"limit\":3\n      }'\n  )\n)['results'] as results;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d742c03-4eb2-458e-a67e-8d6b25f91151",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "\n#query it with Python\nfrom snowflake.core import Root\nfrom snowflake.snowpark.context import get_active_session\nimport streamlit as st\nimport json\nimport pandas as pd\n\nsession = get_active_session()\n\nprompt=\"Any goggles review?\"\n\nroot = Root(session)\n\n# query service\nsvc = (root\n  .databases[\"AVALANCHE\"]\n  .schemas[\"AVALANCHE\"]\n  .cortex_search_services[\"AVALANCHE_SEARCH_SERVICE\"]\n)\n\nresp = svc.search(\n  query=prompt,\n  columns=[\"CHUNK\", \"file_name\"],\n  limit=3\n).to_json()\n\n#optional - I just like the way this looks...\njson_conv = json.loads(resp) if isinstance(resp, str) else resp\nsearch_df = pd.json_normalize(json_conv['results'])\n\n#st.write(search_df)\nfor _, row in search_df.iterrows():\n    st.write(f\"**{row['CHUNK']}**\")\n    st.caption(row['file_name'])\n    st.write('---')",
   "execution_count": null
  }
 ]
}