{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "yoff7uddr2yzlgou3wdc",
   "authorId": "7086005961584",
   "authorName": "DATAPROFESSOR",
   "authorEmail": "hellodataprofessor@gmail.com",
   "sessionId": "127cf661-52e8-422a-9d6f-f6718e031fd9",
   "lastEditTime": 1745948782914
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38532d44-0ee8-48d3-b9d6-760c29ce0ff8",
   "metadata": {
    "name": "md_title",
    "collapsed": false
   },
   "source": "# Setting up the RAG Pipeline using Cortex Search\n\nIn this notebook, we're setting up the RAG pipeline using the Avalanche customer review data from the previously prepared data stored in the stage. \n\nBy the end of the tutorial, we'll have a RAG pipeline ready to allow us to ask any questions about the data."
  },
  {
   "cell_type": "markdown",
   "id": "bdfcb177-a3a2-45cb-b459-2e411d880e11",
   "metadata": {
    "name": "md_install",
    "collapsed": false
   },
   "source": "## Install prerequisite\n\nMake sure to have `snowflake.core` library installed by clicking on Packages in the top menu and enter `snowflake.core` into the text box."
  },
  {
   "cell_type": "markdown",
   "id": "86d33a5a-ab8a-4a97-bc4c-0cfab8179d50",
   "metadata": {
    "name": "md_list_data",
    "collapsed": false
   },
   "source": "## List staged data\n\nHere, we're listing the contents of the staged data at `@AVALANCHE_DB.AVALANCHE_SCHEMA.CUSTOMER_REVIEWS`"
  },
  {
   "cell_type": "code",
   "id": "bd1b575d-341f-48c6-95d0-0ec6bee89c78",
   "metadata": {
    "language": "sql",
    "name": "sql_list_data"
   },
   "outputs": [],
   "source": "ls @avalanche_db.avalanche_schema.customer_reviews;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "721b4bdd-fec8-4d48-9e5b-56aafc52a54d",
   "metadata": {
    "name": "md_parse_content",
    "collapsed": false
   },
   "source": "## Parse content\n\nHere, we're extracting or parsing content from the `DOCX` files stored in stage by leveraging the Cortex `PARSE_DOCUMENT()` function.\n\nThe parsed content will be stored at a newly created table at  `AVALANCHE.AVALANCHE.PARSED_CONTENT`."
  },
  {
   "cell_type": "code",
   "id": "4170f929-6275-4055-b025-6c760ee7e109",
   "metadata": {
    "language": "sql",
    "name": "sql_parse_content",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE AVALANCHE_DB.AVALANCHE_SCHEMA.PARSED_CONTENT AS SELECT \n      relative_path,\n      TO_VARCHAR(\n        SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n          @avalanche_db.avalanche_schema.customer_reviews, \n          relative_path, \n          {'mode': 'LAYOUT'}\n        ) :content\n      ) AS parsed_text\n    FROM directory(@avalanche_db.avalanche_schema.customer_reviews)\n    WHERE relative_path LIKE '%.docx'",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f4341b3-891c-41ee-a08c-7e012e10fa8b",
   "metadata": {
    "name": "md_parse_content_2",
    "collapsed": false
   },
   "source": "## Query parsed content\n\nLet's now query the parsed content stored in the `PARSED_CONTENT` table."
  },
  {
   "cell_type": "code",
   "id": "be400d91-e883-42d7-b26e-eb78a7219479",
   "metadata": {
    "language": "sql",
    "name": "sql_parse_content_2",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM AVALANCHE_DB.AVALANCHE_SCHEMA.PARSED_CONTENT LIMIT 5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "535af127-985c-4bcb-8ffc-c0230fd8cc07",
   "metadata": {
    "name": "md_chunk_content",
    "collapsed": false
   },
   "source": "## Chunk content\n\nWe'll now take the parsed data and perform chunking via `SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER`.\n\nChunked content are stored at `AVALANCHE.AVALANCHE.CHUNKED_CONTENT`."
  },
  {
   "cell_type": "code",
   "id": "1004e517-c9d3-4217-b091-74fc5750865f",
   "metadata": {
    "language": "sql",
    "name": "sql_chunk_content",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE AVALANCHE_DB.AVALANCHE_SCHEMA.CHUNKED_CONTENT (\n    file_name VARCHAR,\n    CHUNK VARCHAR\n);\n\nINSERT INTO AVALANCHE_DB.AVALANCHE_SCHEMA.CHUNKED_CONTENT (file_name, CHUNK)\nSELECT\n    relative_path,\n    c.value AS CHUNK\nFROM\n    AVALANCHE_DB.AVALANCHE_SCHEMA.PARSED_CONTENT,\n    LATERAL FLATTEN( input => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER (\n        parsed_text,\n        'markdown',\n        1800,\n        250\n    )) c;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4b42dcf8-70ae-4c56-8171-5e56fea613ad",
   "metadata": {
    "name": "md_chunk_content_2",
    "collapsed": false
   },
   "source": "## Query chunked content\n\nLet's now query the chunked content stored in the `AVALANCHE_DB.AVALANCHE_SCHEMA.CHUNKED_CONTENT` table."
  },
  {
   "cell_type": "code",
   "id": "42c3fd5b-5002-4b46-b0d7-232327da5fc4",
   "metadata": {
    "language": "sql",
    "name": "sql_chunk_content_2",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM AVALANCHE_DB.AVALANCHE_SCHEMA.CHUNKED_CONTENT LIMIT 10",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8d6f71f1-51ac-4bc1-8bd0-45c28dc97cac",
   "metadata": {
    "name": "md_create_rag",
    "collapsed": false
   },
   "source": "## Create RAG pipeline with Cortex Search\n\nHere, we'll create the RAG pipeline with Cortex Search via `CORTEX SEARCH SERVICE` and this is made available at `AVALANCHE_DB.AVALANCHE_SCHEMA.AVALANCHE_SEARCH_SERVICE`."
  },
  {
   "cell_type": "code",
   "id": "72983c92-cd1f-4b6d-bc06-c65b038f3999",
   "metadata": {
    "language": "sql",
    "name": "sql_create_rag",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE CORTEX SEARCH SERVICE AVALANCHE_DB.AVALANCHE_SCHEMA.AVALANCHE_SEARCH_SERVICE\n    ON chunk\n    WAREHOUSE = compute_wh\n    TARGET_LAG = '1 minute'\n    EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n    AS (\n    SELECT\n        file_name,\n        chunk\n    FROM AVALANCHE_DB.AVALANCHE_SCHEMA.CHUNKED_CONTENT\n    );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "340ae19f-deb0-4328-8838-10a76b0bced8",
   "metadata": {
    "name": "md_search_query",
    "collapsed": false
   },
   "source": "## RAG query\n\nNow comes the fun part, we're now going to utilize the RAG pipeline by asking a question.\n\nFirst, let's use SQL to perform a RAG query using `SNOWFLAKE.CORTEX.SEARCH_PREVIEW()`."
  },
  {
   "cell_type": "code",
   "id": "988ea569-dba2-402e-a727-ecb28d10e789",
   "metadata": {
    "language": "sql",
    "name": "sql_search_query"
   },
   "outputs": [],
   "source": "-- Query it with SQL\nSELECT PARSE_JSON(\n  SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n      'AVALANCHE_DB.AVALANCHE_SCHEMA.AVALANCHE_SEARCH_SERVICE',\n      '{\n         \"query\": \"Any goggles review?\",\n         \"columns\":[\n            \"file_name\",\n            \"CHUNK\"\n         ],\n         \"limit\":3\n      }'\n  )\n)['results'] as results;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "09cc7729-d203-4861-a666-0626ad8fb707",
   "metadata": {
    "name": "md_search_query_2",
    "collapsed": false
   },
   "source": "Now, let's perform the RAQ query in Python."
  },
  {
   "cell_type": "code",
   "id": "1d742c03-4eb2-458e-a67e-8d6b25f91151",
   "metadata": {
    "language": "python",
    "name": "py_search_query"
   },
   "outputs": [],
   "source": "# Query it with Python\nfrom snowflake.core import Root\nfrom snowflake.snowpark.context import get_active_session\nimport streamlit as st\nimport json\nimport pandas as pd\n\nsession = get_active_session()\n\nprompt=\"Any goggles review?\"\n\nroot = Root(session)\n\n# Query service\nsvc = (root\n  .databases[\"AVALANCHE_DB\"]\n  .schemas[\"AVALANCHE_SCHEMA\"]\n  .cortex_search_services[\"AVALANCHE_SEARCH_SERVICE\"]\n)\n\nresp = svc.search(\n  query=prompt,\n  columns=[\"CHUNK\", \"file_name\"],\n  limit=3\n).to_json()\n\n# JSON formatting\njson_conv = json.loads(resp) if isinstance(resp, str) else resp\nsearch_df = pd.json_normalize(json_conv['results'])\n\nfor _, row in search_df.iterrows():\n    st.write(f\"**{row['CHUNK']}**\")\n    st.caption(row['file_name'])\n    st.write('---')",
   "execution_count": null
  }
 ]
}
